\section{Estimaciones del parámetro b}
\subsection{Método de momentos}
Planteemos el momento muestral de primer orden:

\begin{align*}
	&E(X) = \frac{\sum_{i=1}^{n}X_{i}}{n} \\
\end{align*}

Como $X_{1}, \dots, X_{n}$ es una muestra aleatoria de una distribución $U[0, b]$, podemos igualar el primer momento muestral a la esperanza de la distribución con parámetros $0$ y $b$:

\begin{align*}
	\frac{\sum_{i=1}^{n}X_{i}}{n} = E(X) &\implies \\
	\frac{\sum_{i=1}^{n}X_{i}}{n} = \frac{b}{2} &\implies \\
	\bar{X} = \frac{b}{2} &\implies \\
	2 \cdot \bar{X} = b&
\end{align*}

De esta manera, obtenemos la expresión del estimador del parámetro $b$:

\begin{align*}
	\hat{b}_{mom} = 2 \bar{X}_{n}
\end{align*}

\subsection{Estimador de Máxima Verosimilitud}
Sabemos que la función de densidad de una distribución uniforme es:
$$f_{X}(x)=\frac{1}{b - a}I_{(a, b)}(x)$$
Sabiendo que $a = 0$ y que $a < b$ por propiedades de distribución uniforme, procederemos a maximizar $f_{X}(x)$ para estimar el valor de máxima verosimilitud de $b$:

\begin{align*}
	L(b) &= \prod_{i=1}^{n}\frac{1}{b - a}I_{(a, b)}(x) \\
	\iff L(b) &= \prod_{i=1}^{n}\frac{1}{b - 0}I_{(a, b)}(x) \\
	\iff L(b) &= \prod_{i=1}^{n}\frac{1}{b}I_{(x_{i}, +\infty)}(b)
\end{align*}

Por lo tanto:

\begin{center}
\begin{displaymath}
L(b) = \left\{
\begin{array}{l l}
			\frac{1}{b^n} & \text{si }max\{x_{1}, \dots, x_{n}\} < b\\
			0 & \text{sino}
\end{array}
\right.
\end{displaymath}
\end{center}

Esto implica que $\frac{1}{b^n}$ es máximo cuando $b^n$ es mínimo. Luego, el valor más chico posible de $b$ es el valor más grande que haya tomado algún resultado en la muestra $X_{1}, \dots, X_{n}$. Porque si $b$ fuese menor a alguno de ellos, la probabilidad total sería nula (por contener un elemento fuera del rango de la distribución). Finalmente, tenemos un valor para nuestro estimador:
$$\hat{b}_{mv} = max\{x_{1}, \dots, x_{n}\}$$